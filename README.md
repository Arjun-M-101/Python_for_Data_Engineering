```markdown
# ğŸ Python for Data Engineering

## ğŸš€ Overview
This repository is a structured learning and practice environment for mastering **Python, Pandas, PySpark, Modularization, Logging, and ETL pipelines** from a Data Engineering perspective.  

It is organized into phases, each focusing on a specific skill set, culminating in **batch & streaming pipeline projects**.

---

## ğŸ“‚ Folder Structure
```
python_for_data_engineering/
â”œâ”€â”€ 0.Phase_0_Basic_Python.py        # User input/output, data types, loops, functions
â”œâ”€â”€ 1.Phase_1_Core_Python.py         # Collections, comprehensions, file handling, error handling
â”œâ”€â”€ 2.Phase_2_Pandas.py              # Pandas for DE: cleaning, transformations, joins, aggregations
â”œâ”€â”€ 3.Phase_3_PySpark.py             # PySpark for DE: schema, cleaning, joins, aggregations, ETL
â”œâ”€â”€ 4.Phase_4_Modularization_OOPs_Logging.py # Modular ETL with utils + logging
â”œâ”€â”€ 5.Phase_5_Projects.py            # Batch & streaming pipeline projects (reference)
â”œâ”€â”€ utils.py                         # Helper functions for ETL (extract, transform, load)
â”œâ”€â”€ logger.py                        # Logging setup for ETL pipelines
â”œâ”€â”€ etl_pipeline.py                  # OOP-based ETL pipeline class
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ etl.log                          # Log file generated during ETL runs
â”œâ”€â”€ input/                           # Input datasets (CSV, JSON, TXT)
â”‚   â”œâ”€â”€ people.csv
â”‚   â”œâ”€â”€ employees.csv
â”‚   â”œâ”€â”€ data.json
â”‚   â””â”€â”€ notes.txt
â”œâ”€â”€ data_file/                       # Raw & processed sales data
â”‚   â”œâ”€â”€ raw_sales.csv
â”‚   â””â”€â”€ processed_sales.csv
â”œâ”€â”€ output/                          # ETL outputs (CSV, JSON, Excel, Parquet)
â”‚   â”œâ”€â”€ final_sales_data.csv/json/xlsx
â”‚   â”œâ”€â”€ employees_cleaned.csv/json/parquet
â”‚   â”œâ”€â”€ processed_people.csv
â”‚   â”œâ”€â”€ transformed.csv
â”‚   â”œâ”€â”€ final_data/                  # Parquet outputs
â”‚   â””â”€â”€ partitioned_data/            # Partitioned parquet outputs by Region
â””â”€â”€ README.md                        # Project documentation
```

---

## ğŸ› ï¸ Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/Arjun-M-101/Python_for_Data_Engineering.git
   cd python_for_data_engineering
   ```

2. **Create virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate   # Linux/Mac
   venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**
   ```bash
   python -c "import pandas, pyspark; print('âœ… Setup OK')"
   ```

---

## ğŸ“– Phase Breakdown

### Phase 0 â€“ Basic Python
- Input/output, conditionals, loops
- Functions, mutability, lambda, map/filter/reduce
- Time complexity basics

### Phase 1 â€“ Core Python
- Lists, dicts, comprehensions
- File handling (CSV, JSON, TXT)
- Error handling with try/except/finally
- Modularization example (`utils.py`)

### Phase 2 â€“ Pandas for DE
- Data cleaning (nulls, types, duplicates)
- Transformations (new columns, filtering, feature engineering)
- Joins, groupby, aggregations
- Export to CSV, JSON, Excel
- Mini ETL pipeline with Pandas

### Phase 3 â€“ PySpark for DE
- SparkSession setup
- Schema management
- Cleaning (dropDuplicates, fillna, trim)
- Transformations (withColumn, filter, joins)
- Aggregations, sorting, conditional logic
- UDFs, date/time functions
- Writing outputs (CSV, JSON, Parquet, partitioned)
- Endâ€‘toâ€‘end PySpark ETL pipeline

### Phase 4 â€“ Modularization, OOPs, Logging
- `utils.py`: extract, transform, load functions
- `logger.py`: centralized logging setup
- `etl_pipeline.py`: OOPâ€‘based ETL pipeline class

### Phase 5 â€“ Projects
- Batch & streaming pipelines (reference)
- Realâ€‘world DE scenarios

---

## ğŸ“¦ Outputs
- **CSV/JSON/Excel** exports for cleaned datasets  
- **Parquet** outputs for scalable storage  
- **Partitioned Parquet** by region for analytics  
- **Logs** stored in `etl.log` for monitoring  

---

## âœ… Key Learnings
- Python fundamentals for DE
- Pandas for batch data processing
- PySpark for scalable distributed pipelines
- Modularization & OOP design for ETL
- Logging for observability
- Endâ€‘toâ€‘end ETL pipeline implementation

---